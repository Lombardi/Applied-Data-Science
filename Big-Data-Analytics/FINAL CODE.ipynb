{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "import pyspark\n",
    "from pyspark.ml import feature, regression, Pipeline, classification, evaluation\n",
    "from pyspark.sql import functions as fn, Row\n",
    "from pyspark import sql\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOADING DATA AND PREPROCESSING ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98643"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_df = spark.read.csv('flights.csv', header = True)\n",
    "flights_df = flights_df.sample(False, 0.0170)\n",
    "flights_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = [\n",
    "'SCHEDULED_DEPARTURE',\n",
    "'DEPARTURE_TIME', \n",
    "'WHEELS_OFF',\n",
    "'DISTANCE',\n",
    "'WHEELS_ON',\n",
    "'SCHEDULED_ARRIVAL',\n",
    "'ARRIVAL_TIME',\n",
    "'ARRIVAL_DELAY',\n",
    "'CANCELLED',\n",
    "'CANCELLATION_REASON',\n",
    "'YEAR',\n",
    "'TAXI_OUT',\n",
    "'ELAPSED_TIME',\n",
    "'AIR_TIME','TAXI_IN',\n",
    "'AIR_SYSTEM_DELAY',\n",
    "'SECURITY_DELAY',\n",
    "'AIRLINE_DELAY',\n",
    "'LATE_AIRCRAFT_DELAY',\n",
    "'WEATHER_DELAY',\n",
    "'DIVERTED']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_df =  flights_df.drop(*drop_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- MONTH: string (nullable = true)\n",
      " |-- DAY: string (nullable = true)\n",
      " |-- DAY_OF_WEEK: string (nullable = true)\n",
      " |-- AIRLINE: string (nullable = true)\n",
      " |-- FLIGHT_NUMBER: string (nullable = true)\n",
      " |-- TAIL_NUMBER: string (nullable = true)\n",
      " |-- ORIGIN_AIRPORT: string (nullable = true)\n",
      " |-- DESTINATION_AIRPORT: string (nullable = true)\n",
      " |-- DEPARTURE_DELAY: string (nullable = true)\n",
      " |-- SCHEDULED_TIME: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check schema #\n",
    "flights_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change data types #\n",
    "flights_df = flights_df.withColumn('DEPARTURE_DELAY_num',fn.col('DEPARTURE_DELAY').cast('int')).drop('DEPARTURE_DELAY')\n",
    "flights_df = flights_df.withColumn('SCHEDULED_TIME_num',fn.col('SCHEDULED_TIME').cast('int')).drop('SCHEDULED_TIME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----------+-------+-------------+-----------+--------------+-------------------+-------------------+------------------+\n",
      "|MONTH|DAY|DAY_OF_WEEK|AIRLINE|FLIGHT_NUMBER|TAIL_NUMBER|ORIGIN_AIRPORT|DESTINATION_AIRPORT|DEPARTURE_DELAY_num|SCHEDULED_TIME_num|\n",
      "+-----+---+-----------+-------+-------------+-----------+--------------+-------------------+-------------------+------------------+\n",
      "|    1|  1|          4|     AA|          196|     N3FBAA|           MIA|                PHL|                  4|               164|\n",
      "+-----+---+-----------+-------+-------------+-----------+--------------+-------------------+-------------------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "98643"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_df.show(1)\n",
    "flights_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use bucketizer function to discretize continuous columns #\n",
    "\n",
    "time_splits = [0.0, 500.0, 1200.0, 1800.0, 2000.0, 2359.0]\n",
    "delay_splits = [float('-inf'), 0, float('inf')]\n",
    "\n",
    "time_bucketizer = feature.Bucketizer(splits = time_splits, inputCol = 'SCHEDULED_TIME_num', outputCol = 'SCHEDULED_TIME_idx')\n",
    "delay_bucketizer = feature.Bucketizer(splits = delay_splits, inputCol = 'DEPARTURE_DELAY_num', outputCol = 'DEPARTURE_DELAY_idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucketizers = [time_bucketizer, delay_bucketizer]\n",
    "for bucketizer in bucketizers:\n",
    "    flights_df = bucketizer.transform(flights_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----------+-------+-------------+-----------+--------------+-------------------+-------------------+------------------+------------------+-------------------+\n",
      "|MONTH|DAY|DAY_OF_WEEK|AIRLINE|FLIGHT_NUMBER|TAIL_NUMBER|ORIGIN_AIRPORT|DESTINATION_AIRPORT|DEPARTURE_DELAY_num|SCHEDULED_TIME_num|SCHEDULED_TIME_idx|DEPARTURE_DELAY_idx|\n",
      "+-----+---+-----------+-------+-------------+-----------+--------------+-------------------+-------------------+------------------+------------------+-------------------+\n",
      "|    1|  1|          4|     AA|          196|     N3FBAA|           MIA|                PHL|                  4|               164|               0.0|                1.0|\n",
      "+-----+---+-----------+-------+-------------+-----------+--------------+-------------------+-------------------+------------------+------------------+-------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "98643"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_df.show(1)\n",
    "flights_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert indices back to appropriate strings #\n",
    "\n",
    "time_labels = ['Late Night','Morning','Afternoon','Evening','Night']\n",
    "delay_labels = ['On Time','Delayed']\n",
    "\n",
    "time_converter = feature.IndexToString(inputCol = \"SCHEDULED_TIME_idx\", outputCol = \"SCHEDULED_TIME_str\", labels = time_labels)\n",
    "delay_converter = feature.IndexToString(inputCol = \"DEPARTURE_DELAY_idx\", outputCol = \"DEPARTURE_DELAY_str\", labels = delay_labels)\n",
    "\n",
    "index_converters = [time_converter, delay_converter]\n",
    "\n",
    "for converter in index_converters:\n",
    "    flights_df = converter.transform(flights_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all intermediate columns and rename processed columns back to their original label #\n",
    "\n",
    "flights_df = flights_df.drop('SCHEDULED_TIME_idx','DEPARTURE_DELAY_idx','DEPARTURE_DELAY_num','SCHEDULED_TIME_num')\n",
    "flights_df = flights_df.withColumnRenamed('SCHEDULED_TIME_str','SCHEDULED_TIME')\\\n",
    ".withColumnRenamed('DEPARTURE_DELAY_str','DEPARTURE_DELAY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----------+-------+-------------+-----------+--------------+-------------------+--------------+---------------+\n",
      "|MONTH|DAY|DAY_OF_WEEK|AIRLINE|FLIGHT_NUMBER|TAIL_NUMBER|ORIGIN_AIRPORT|DESTINATION_AIRPORT|SCHEDULED_TIME|DEPARTURE_DELAY|\n",
      "+-----+---+-----------+-------+-------------+-----------+--------------+-------------------+--------------+---------------+\n",
      "|    1|  1|          4|     AA|          196|     N3FBAA|           MIA|                PHL|    Late Night|        Delayed|\n",
      "+-----+---+-----------+-------+-------------+-----------+--------------+-------------------+--------------+---------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "98643"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_df.show(1)\n",
    "flights_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONTH : 0\n",
      "DAY : 0\n",
      "DAY_OF_WEEK : 0\n",
      "AIRLINE : 0\n",
      "FLIGHT_NUMBER : 0\n",
      "TAIL_NUMBER : 264\n",
      "Filtering...\n",
      "ORIGIN_AIRPORT : 0\n",
      "DESTINATION_AIRPORT : 0\n",
      "SCHEDULED_TIME : 0\n",
      "DEPARTURE_DELAY : 1205\n",
      "Filtering...\n"
     ]
    }
   ],
   "source": [
    "#  Check for null values in each column and filter results if it returns a value greater than zero#\n",
    "for col in flights_df.columns:\n",
    "    print(col, ':', flights_df.filter(flights_df[col].isNull()).count())\n",
    "    if flights_df.filter(flights_df[col].isNull()).count() > 0:\n",
    "        print('Filtering...')\n",
    "        flights_df = flights_df.filter(flights_df[col].isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97174"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out all of the airports that are represented as numbers #\n",
    "\n",
    "expr = \"[a-zA-Z]+\"\n",
    "airport_cols = ['ORIGIN_AIRPORT','DESTINATION_AIRPORT']\n",
    "\n",
    "for col in airport_cols:\n",
    "    flights_df = flights_df.filter(flights_df[col].rlike(expr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89011"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the final count of records #\n",
    "flights_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the clean data to a new .csv for ease of future use #\n",
    "flights_df.write.csv('flights_clean.csv', header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL BUILDING AND VALIDATION ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the clean data #\n",
    "flights_df_clean = spark.read.csv('flights_clean.csv', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----------+-------+-------------+-----------+--------------+-------------------+--------------+---------------+\n",
      "|MONTH|DAY|DAY_OF_WEEK|AIRLINE|FLIGHT_NUMBER|TAIL_NUMBER|ORIGIN_AIRPORT|DESTINATION_AIRPORT|SCHEDULED_TIME|DEPARTURE_DELAY|\n",
      "+-----+---+-----------+-------+-------------+-----------+--------------+-------------------+--------------+---------------+\n",
      "|    6| 17|          3|     DL|         1372|     N692DL|           ATL|                JAX|    Late Night|        On Time|\n",
      "+-----+---+-----------+-------+-------------+-----------+--------------+-------------------+--------------+---------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_df_clean.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## THE PIPELINE (indexding, encoding, and assembling) ##\n",
    "\n",
    "categorical_cols = ['DAY', 'MONTH', 'DAY_OF_WEEK', 'AIRLINE','FLIGHT_NUMBER','TAIL_NUMBER','ORIGIN_AIRPORT',\n",
    "                    'DESTINATION_AIRPORT','SCHEDULED_TIME']\n",
    "\n",
    "label_indexer = [feature.StringIndexer(inputCol = \"DEPARTURE_DELAY\",\n",
    "                                     outputCol = \"DEPARTURE_DELAY_index\", handleInvalid = \"skip\")]\n",
    "\n",
    "indexers = [feature.StringIndexer(inputCol = column, \n",
    "                                  outputCol = \"{0}_index\".format(column), handleInvalid = \"skip\")\n",
    "            for column in categorical_cols\n",
    "           ]\n",
    "\n",
    "encoders = [feature.OneHotEncoder(dropLast=False, inputCol = indexer.getOutputCol(),\n",
    "                                         outputCol = \"{0}_encoded\".format(indexer.getOutputCol()))\n",
    "          for indexer in indexers\n",
    "           ]\n",
    "\n",
    "assembler = [feature.VectorAssembler(inputCols = [encoder.getOutputCol() for encoder in encoders],\n",
    "                                     outputCol = \"features\")\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the preprocessing pipeline, then fit and transform it to the entire dataset #\n",
    "# Also rename the Departure Delay index to 'label' for ease of future use #\n",
    "\n",
    "flights_pipe = Pipeline(stages=indexers + label_indexer + encoders +assembler)\n",
    "flights_model=flights_pipe.fit(flights_df_clean)\n",
    "flights_transformed = flights_model.transform(flights_df_clean).withColumn('label',fn.col('DEPARTURE_DELAY_index'))\\\n",
    ".drop('DEPARTURE_DELAY_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a training, validation, and test split #\n",
    "training_df, validation_df, testing_df = flights_transformed.randomSplit([0.6, 0.3, 0.1], seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a logistic regression model (default params) using the training data once the preprocessing pipeline has been #\n",
    "# transformed against it\n",
    "\n",
    "lr = classification.LogisticRegression(featuresCol='features',labelCol='label',\n",
    "                                      regParam = 0.01, elasticNetParam = 0.5, maxIter = 10)\n",
    "lr_model = lr.fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6111528432173913"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform the model against the validation dataset and print out the accuracy #\n",
    "lr_predictions = lr_model.transform(validation_df)\n",
    "\n",
    "evaluator = evaluation.BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "evaluator.evaluate(lr_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an optimization grid in order to test different parameter values #\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "lr_paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.01, 0.5, 2.0])\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "             .addGrid(lr.maxIter, [1, 5, 10])\n",
    "             .build())\n",
    "\n",
    "# Find best parameters by cross validating #\n",
    "lr_cv = CrossValidator(estimator=lr, estimatorParamMaps=lr_paramGrid, evaluator=evaluator, numFolds=3)\n",
    "lr_cvModel = lr_cv.fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reg Parameter:  0.01\n",
      "Elastic Net Parameter:  0.5\n",
      "Max Iterations:  1\n"
     ]
    }
   ],
   "source": [
    "# Optimized Parameters #\n",
    "print('Reg Parameter: ', lr_cvModel.bestModel._java_obj.getRegParam())\n",
    "print('Elastic Net Parameter: ', lr_cvModel.bestModel._java_obj.getElasticNetParam())\n",
    "print('Max Iterations: ', lr_cvModel.bestModel._java_obj.getMaxIter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6110406171295445"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform the optimized model against the validation dataset and evaluate it #\n",
    "# This is the validation metric we will use #\n",
    "lr_opt_predictions = lr_cvModel.bestModel.transform(validation_df)\n",
    "evaluator = evaluation.BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "evaluator.evaluate(lr_opt_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a linear SVM model to the training dataset with default parameters #\n",
    "\n",
    "svm = classification.LinearSVC(featuresCol='features',labelCol='label')\n",
    "svm_model = svm.fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.571324164777052"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform it against the validation dataset and evaluate #\n",
    "svm_predictions = svm_model.transform(validation_df)\n",
    "evaluator.evaluate(svm_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize parameters (this takes a while) #\n",
    "svm_paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(svm.regParam, [0.0, 0.5, 2.0])\n",
    "             .addGrid(svm.tol, [0.0, 1e-06, .001])\n",
    "             .addGrid(svm.maxIter, [50,100,200])\n",
    "             .build())\n",
    "\n",
    "svm_cv = CrossValidator(estimator=svm, estimatorParamMaps=svm_paramGrid, evaluator=evaluator, numFolds=3)\n",
    "svm_cvModel = svm_cv.fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization Parameter:  2.0\n",
      "Tolerance:  1e-06\n",
      "Max Iterations:  50\n"
     ]
    }
   ],
   "source": [
    "# Optimized Parameters #\n",
    "print('Regularization Parameter: ', svm_cvModel.bestModel._java_obj.getRegParam())\n",
    "print('Tolerance: ', svm_cvModel.bestModel._java_obj.getTol())\n",
    "print('Max Iterations: ',svm_cvModel.bestModel._java_obj.getMaxIter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5870373228893604"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_opt_predictions = svm_cvModel.bestModel.transform(validation_df)\n",
    "evaluator = evaluation.BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "evaluator.evaluate(svm_opt_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a random forest classifier to the training dataset with default parameters #\n",
    "\n",
    "rf = classification.RandomForestClassifier(featuresCol = \"features\", labelCol = \"label\",\n",
    "                                          maxDepth = 10, maxBins = 60, numTrees = 50)\n",
    "rf_model = rf.fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6037013567670825"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate initial model #\n",
    "\n",
    "rf_predictions = rf_model.transform(validation_df)\n",
    "evaluator = evaluation.BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "evaluator.evaluate(rf_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize parameters (this takes a while)#\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "rf_paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf.maxDepth, [5, 10])\n",
    "             .addGrid(rf.maxBins, [20,60])\n",
    "             .addGrid(rf.numTrees, [20, 50])\n",
    "             .build())\n",
    "\n",
    "rf_cv = CrossValidator(estimator=rf, estimatorParamMaps=rf_paramGrid, evaluator=evaluator, numFolds=3)\n",
    "rf_cvModel = rf_cv.fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized Parameters #\n",
    "print('Max Depth: ', rf_cvModel.bestModel._java_obj.getMaxDepth())\n",
    "print('Max Bins: ', rf_cvModel.bestModel._java_obj.getMaxBins())\n",
    "print('Number of Trees: ',rf_cvModel.bestModel._java_obj.getNumTrees())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rf_opt_predictions = rf_cvModel.bestModel.transform(validation_df)\n",
    "evaluator = evaluation.BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "evaluator.evaluate(rf_opt_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a naive bayes model to the training data with default parameters #\n",
    "\n",
    "nb = classification.NaiveBayes()\n",
    "nb_model = nb.fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform it against the validation data #\n",
    "\n",
    "nb_predictions = nb_model.transform(validation_df)\n",
    "evaluator = evaluation.BinaryClassificationEvaluator(labelCol='label')\n",
    "evaluator.evaluate(nb_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "nb_paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(nb.smoothing, [0.05,1,2])\n",
    "             .build())\n",
    "\n",
    "nb_cv = CrossValidator(estimator=nb, estimatorParamMaps=nb_paramGrid, evaluator=evaluator, numFolds=3)\n",
    "nb_cvModel = nb_cv.fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized Parameters #\n",
    "print('Smoothing: ', nb_cvModel.bestModel._java_obj.getSmoothing())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_opt_predictions = nb_cvModel.bestModel.transform(validation_df)\n",
    "evaluator.evaluate(nb_opt_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Summary ###\n",
    "\n",
    "#### Logistic Regression ####\n",
    "- Optimized AUC: **0.613**\n",
    "\n",
    "#### Linear SVM ####\n",
    "- Optimized AUC: **0.605**\n",
    "\n",
    "#### Random Forest ####\n",
    "- Optimized AUC: **0.606**\n",
    "\n",
    "#### Naive Bayes ####\n",
    "- Optimized AUC: **0.590**\n",
    "\n",
    "*Based on these validation metrics, Logistic Regression is the model that we will use on the testing data.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = lr_cvModel.bestModel.transform(testing_df)\n",
    "print('Testing AUC: ', evaluator.evaluate(test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Coefficients ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the feature indices and names from the metadata of the encoded features, and then map the model's feature\n",
    "# coefficients back onto them.\n",
    "\n",
    "from itertools import chain\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "attrs = sorted(\n",
    "    (attr[\"idx\"], attr[\"name\"]) for attr in (chain(*training_df\n",
    "        .schema[\"features\"]\n",
    "        .metadata[\"ml_attr\"][\"attrs\"].values())))\n",
    "\n",
    "mapped_coefficients = [(name, lr_model.coefficients[idx])\n",
    "                      for idx, name in attrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 20 coefficients, and then plot them #\n",
    "\n",
    "top_coefficients = sorted(mapped_coefficients, key = lambda x: x[1], reverse=True)[:20]\n",
    "names = list(zip(*top_coefficients))[0]\n",
    "imp = list(zip(*top_coefficients))[1]\n",
    "x_pos = np.arange(len(names))\n",
    "plt.figure(figsize = (10,8))\n",
    "plt.barh(x_pos[::-1], imp, align = 'center')\n",
    "plt.yticks(x_pos[::-1], names, linespacing = 2)\n",
    "plt.title('Logistic Regression Feature Coefficients')\n",
    "plt.show()\n",
    "\n",
    "plt.savefig('lr_coefficients.pdf', bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Feature Importances ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to the LR model, map the random forest model's feature importances back onto the features\n",
    "# themselves (the mapping can take a while)#\n",
    "\n",
    "mapped_importances = [(name, rf_model.featureImportances[idx])\n",
    " for idx, name in attrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 20 importances, and then plot them #\n",
    "\n",
    "top_importances = sorted(mapped_importances, key = lambda x: x[1], reverse=True)[:20]\n",
    "names = list(zip(*top_importances))[0]\n",
    "imp = list(zip(*top_importances))[1]\n",
    "x_pos = np.arange(len(names))\n",
    "plt.figure(figsize = (10,8))\n",
    "plt.barh(x_pos[::-1], imp, align = 'center')\n",
    "plt.yticks(x_pos[::-1], names, linespacing = 2)\n",
    "plt.title('Random Forest Feature Importances')\n",
    "plt.show()\n",
    "\n",
    "plt.savefig('feature_importances.pdf', bbox_inches=\"tight\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
